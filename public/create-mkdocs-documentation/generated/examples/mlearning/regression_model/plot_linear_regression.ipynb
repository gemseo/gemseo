{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Linear regression.\n\nA [LinearRegressor][gemseo.mlearning.regression.algos.linreg.LinearRegressor] is a linear regression model\nbased on [scikit-learn](https://scikit-learn.org).\n\n!!! info \"See also\"\n\n    You can find more information about building linear models with scikit-learn on\n    [this page](https://scikit-learn.org/stable/modules/linear_model.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nfrom matplotlib import pyplot as plt\nfrom numpy import array\n\nfrom gemseo import create_design_space\nfrom gemseo import create_discipline\nfrom gemseo import sample_disciplines\nfrom gemseo.mlearning import create_regression_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n\nIn this example,\nwe represent the function $f(x)=(6x-2)^2\\sin(12x-4)$\nby the [AnalyticDiscipline][gemseo.disciplines.analytic.AnalyticDiscipline].\n\n!!! quote \"References\"\n      Alexander I. J. Forrester, Andras Sobester, and Andy J. Keane.\n      Engineering design via surrogate modelling: a practical guide. Wiley, 2008.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "discipline = create_discipline(\n    \"AnalyticDiscipline\",\n    name=\"f\",\n    expressions={\"y\": \"(6*x-2)**2*sin(12*x-4)\"},\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and seek to approximate it over the input space\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_space = create_design_space()\ninput_space.add_variable(\"x\", lower_bound=0.0, upper_bound=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To do this,\nwe create a training dataset with 6 equispaced points:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"PYDOE_FULLFACT\", n_samples=6\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Basics\n\n### Training\n\nThen,\nwe train a linear regression model from these samples:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = create_regression_model(\"LinearRegressor\", training_dataset)\nmodel.learn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Prediction\n\nOnce it is built,\nwe can predict the output value of $f$ at a new input point:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_value = {\"x\": array([0.65])}\noutput_value = model.predict(input_value)\noutput_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "as well as its Jacobian value:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "jacobian_value = model.predict_jacobian(input_value)\njacobian_value"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plotting\n\nOf course,\nyou can see that the linear model is no good at all here:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "test_dataset = sample_disciplines(\n    [discipline], input_space, \"y\", algo_name=\"PYDOE_FULLFACT\", n_samples=100\n)\ninput_data = test_dataset.get_view(variable_names=model.input_names).to_numpy()\nreference_output_data = test_dataset.get_view(variable_names=\"y\").to_numpy().ravel()\npredicted_output_data = model.predict(input_data).ravel()\nplt.plot(input_data.ravel(), reference_output_data, label=\"Reference\")\nplt.plot(input_data.ravel(), predicted_output_data, label=\"Regression - Basics\")\nplt.grid()\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Settings\n\nThe [LinearRegressor][gemseo.mlearning.regression.algos.linreg.LinearRegressor] has many options\ndefined in the [LinearRegressor_Settings][gemseo.mlearning.regression.algos.linreg_settings.LinearRegressor_Settings] Pydantic model.\n\n### Intercept\n\nBy default,\nthe linear model is of the form $a_0+a_1x_1+\\ldots+a_dx_d$.\nYou can set the option `fit_intercept` to `False`\nif you want a linear model of the form $a_1x_1+\\ldots+a_dx_d$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = create_regression_model(\n    \"LinearRegressor\", training_dataset, fit_intercept=False, transformer={}\n)\nmodel.learn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! warning\n\n    This notion applies in the space of transformed variables.\n    This is the reason why\n    we removed the default transformers by setting `transformer` to `{}`.\n\nWe can see the impact of this option in the following visualization:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "predicted_output_data_ = model.predict(input_data).ravel()\nplt.plot(input_data.ravel(), reference_output_data, label=\"Reference\")\nplt.plot(input_data.ravel(), predicted_output_data, label=\"Regression - Basics\")\nplt.plot(input_data.ravel(), predicted_output_data_, label=\"Regression - No intercept\")\nplt.grid()\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Regularization\n\nWhen the number of samples is small relative to the input dimension,\nregularization techniques can save you from overfitting\n(a model that is very good at learning but bad at generalization).\nThe `penalty_level` option is a positive real number\ndefining the degree of regularization (default: no regularization).\nBy default,\nthe regularization technique is the ridge penalty (l2 regularization).\nThe technique can be replaced by the lasso penalty (l1 regularization)\nby setting the `l2_penalty_ratio` option to `0.0`.\nWhen `l2_penalty_ratio` is between 0 and 1,\nthe regularization technique is the elastic net penalty,\n*i.e.* a linear combination of ridge and lasso penalty\nparametrized by this `l2_penalty_ratio`.\n\nFor example,\nwe can use the ridge penalty with a level of 1.2\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = create_regression_model(\"LinearRegressor\", training_dataset, penalty_level=1.2)\nmodel.learn()\npredicted_output_data_ = model.predict(input_data).ravel()\nplt.plot(input_data.ravel(), reference_output_data, label=\"Reference\")\nplt.plot(input_data.ravel(), predicted_output_data, label=\"Regression - Basics\")\nplt.plot(input_data.ravel(), predicted_output_data_, label=\"Regression - Ridge(1.2)\")\nplt.grid()\nplt.legend()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the coefficient of the linear model is lower due to the penalty.\n\n!!! note\n\n    In the case of a model with many inputs,\n    we could have used the lasso penalty\n    and seen that some coefficients would have been set to zero.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}