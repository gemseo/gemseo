{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Plug a surrogate discipline in a Scenario.\n\nIn this section we describe the usage of surrogate model in GEMSEO,\nwhich is implemented in the\n[SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline] class.\n\nA [SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline]\ncan be used to substitute a\n[Discipline][gemseo.core.discipline.discipline.Discipline] within a\n[BaseScenario][gemseo.scenarios.base_scenario.BaseScenario]. This\n[SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline]\nis an evaluation of the [Discipline][gemseo.core.discipline.discipline.Discipline]\nand is faster to compute than the original one. It relies on a\n[BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor].\nThis comes at the price of computing a DOE\non the original [Discipline][gemseo.core.discipline.discipline.Discipline],\nand validating the approximation. The\ncomputations from which the approximation is built can be available, or can be\nbuilt using GEMSEO's DOE capabilities.\nSee these [Sobieski][mdf-based-doe-on-the-sobieski-ssbj-test-case]\nand [Sellar][a-from-scratch-example-on-the-sellar-problem] examples.\n\nIn GEMSEO, the data used to build the surrogate model is taken from a\n[Dataset][gemseo.datasets.dataset.Dataset] containing both inputs and\noutputs of the DOE. This\n[Dataset][gemseo.datasets.dataset.Dataset] may have been generated by\nGEMSEO from a cache, using the\n[BaseCache.to_dataset()][gemseo.caches.base_cache.BaseCache.to_dataset] method,\nfrom a database, using the\n[OptimizationProblem.to_dataset()][gemseo.algos.optimization_problem.OptimizationProblem.to_dataset] method,\nor from a NumPy array or\na text file using the\n[Dataset.from_array()][gemseo.datasets.dataset.Dataset.from_array] and\n[Dataset.from_txt()][gemseo.datasets.dataset.Dataset.from_txt].\n\nThen, the surrogate discipline can be used as any other discipline in a\n[MDOScenario][gemseo.scenarios.mdo_scenario.MDOScenario],\na [DOEScenario][gemseo.scenarios.doe_scenario.DOEScenario],\nor a [BaseMDA][gemseo.mda.base_mda.BaseMDA].\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nfrom numpy import array\nfrom numpy import hstack\nfrom numpy import vstack\n\nfrom gemseo import create_discipline\nfrom gemseo import create_scenario\nfrom gemseo import create_surrogate\nfrom gemseo import sample_disciplines\nfrom gemseo.datasets.io_dataset import IODataset\nfrom gemseo.problems.mdo.sobieski.core.design_space import SobieskiDesignSpace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a surrogate scenario\n\n### Create the training dataset\n\nIf you already have available data from a DOE produced externally,\nit is possible to create a [Dataset][gemseo.datasets.dataset.Dataset] and Step 1\nends here.\nFor example, let us consider a synthetic dataset, with $x$\nas input and $y$ as output, described as a NumPy array.\nThen, we store these data in a [Dataset][gemseo.datasets.dataset.Dataset]:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "variables = [\"x\", \"y\"]\nsizes = {\"x\": 1, \"y\": 1}\ngroups = {\"x\": \"inputs\", \"y\": \"outputs\"}\ndata = vstack((\n    hstack((array([1.0]), array([1.0]))),\n    hstack((array([2.0]), array([2.0]))),\n))\nsynthetic_dataset = IODataset.from_array(data, variables, sizes, groups)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If you do not have available data,the following paragraphs of Step 1 concern you.\n\nHere, we illustrate the generation of the training data using a\n[DOEScenario][gemseo.scenarios.doe_scenario.DOEScenario],\nsimilarly to [this example][mdf-based-doe-on-the-sobieski-ssbj-test-case],\nwhere more details are given.\n\nIn this basic example, a [Discipline][gemseo.core.discipline.discipline.Discipline]\ncomputing the mission\nperformance (range) in the [Sobieski's SSBJ problem][sobieskis-ssbj-test-case] is\nsampled with a [DOEScenario][gemseo.scenarios.doe_scenario.DOEScenario]. Then,\nthe generated database is used to\nbuild a [SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline].\n\nBut more complex scenarios can be used in the same way: complete optimization\nprocesses or MDAs can be replaced by their surrogate counterparts. The right\ncache or database shall then be used to build the\n[SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline],\nbut the main logic won't differ from this\nexample.\n\nFirstly, we create the [Discipline][gemseo.core.discipline.discipline.Discipline]\nby means of the API function\n[create_discipline()][gemseo.create_discipline]:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "discipline = create_discipline(\"SobieskiMission\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we read the [DesignSpace][gemseo.algos.design_space.DesignSpace] of the\n[Sobieski's SSBJ problem][sobieskis-ssbj-test-case]\nand keep only the inputs of the mission discipline as inputs of the DOE,\nnamely `\"x_shared\"`, `\"y_24\"` and `\"y_34\"`:\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "design_space = SobieskiDesignSpace()\ndesign_space = design_space.filter([\"x_shared\", \"y_24\", \"y_34\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From this [Discipline][gemseo.core.discipline.discipline.Discipline] and this\n[DesignSpace][gemseo.algos.design_space.DesignSpace],\nwe can generate 30 samples by means of the\n[sample_disciplines()][gemseo.sample_disciplines] function\nwith the LHS algorithm:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "mission_dataset = sample_disciplines(\n    [discipline], design_space, \"y_4\", algo_name=\"PYDOE_LHS\", n_samples=30\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! info \"See also\"\n\n    In this tutorial, the DOE is based on [pyDOE](https://pythonhosted.org/pyDOE/),\n    However, several other designs are available,\n    based on the package or [OpenTURNS](https://openturns.github.io/www/).\n    Some examples of these designs are plotted\n    in [this page][doe-algorithms].  To list the available DOE algorithms in the\n    current GEMSEO configuration, use\n    [get_available_doe_algorithms()][gemseo.get_available_doe_algorithms].\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the [SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline]\n\nFrom this [Dataset][gemseo.datasets.dataset.Dataset],\nwe can build a [SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline]\nof the [Discipline][gemseo.core.discipline.discipline.Discipline].\n\nIndeed, by means of the API function [create_surrogate()][gemseo.create_surrogate],\nwe create the [SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline]\nfrom the dataset,\nwhich can be executed as any other discipline.\n\nPrecisely,\nby means of the API function [create_surrogate()][gemseo.create_surrogate],\nwe create a [SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline]\nrelying on a [LinearRegressor][gemseo.mlearning.regression.algos.linreg.LinearRegressor]\nand inheriting from [Discipline][gemseo.core.discipline.discipline.Discipline]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "synthetic_surrogate = create_surrogate(\"LinearRegressor\", synthetic_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!!! info \"See also\"\n\n    Note that a subset of the inputs and outputs to be used to build the\n    [SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline]\n    may be specified by the user if needed,\n    mainly to avoid unnecessary computations.\n\nThen, we execute it as any [Discipline][gemseo.core.discipline.discipline.Discipline]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_data = {\"x\": array([2.0])}\nout = synthetic_surrogate.execute(input_data)\nout[\"y\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In our study case, from the DOE built at Step 1,\nwe build a [RBFRegressor][gemseo.mlearning.regression.algos.rbf.RBFRegressor]\nof $y_4$\nrepresenting the range in function of $L/D$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "range_surrogate = create_surrogate(\"RBFRegressor\", mission_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Use the [SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline]\nin MDO\n\nThe obtained [SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline]\ncan be used in any\n[BaseScenario][gemseo.scenarios.base_scenario.BaseScenario], such as a\n[DOEScenario][gemseo.scenarios.doe_scenario.DOEScenario]\nor [MDOScenario][gemseo.scenarios.mdo_scenario.MDOScenario].\nWe see here that the\n[Discipline.execute()][gemseo.core.discipline.discipline.Discipline.execute]\nmethod can be used as in\nany other discipline to compute the outputs for given inputs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in range(5):\n    lod = i * 2.0\n    y_4_pred = range_surrogate.execute({\"y_24\": array([lod])})[\"y_4\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we can build and execute an optimization scenario from it.\nThe design variables are $y_24$. The Jacobian matrix is computed by finite\ndifferences by default for surrogates, except for the\n[SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline] relying on\n[LinearRegressor][gemseo.mlearning.regression.algos.linreg.LinearRegressor] which has\nan analytical (and constant) Jacobian.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "design_space = design_space.filter([\"y_24\"])\nscenario = create_scenario(\n    range_surrogate,\n    \"y_4\",\n    design_space,\n    formulation_name=\"DisciplinaryOpt\",\n    maximize_objective=True,\n)\nscenario.execute(algo_name=\"L-BFGS-B\", max_iter=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Available surrogate models\n\nCurrently, the following surrogate models are available:\n\n- Linear regression,\n  based on the [Scikit-learn](http://scikit-learn.org/stable/) library,\n  for that use the\n  [LinearRegressor][gemseo.mlearning.regression.algos.linreg.LinearRegressor] class.\n- Polynomial regression,\n  based on the [Scikit-learn](http://scikit-learn.org/stable/) library,\n  for that use the\n  [PolynomialRegressor][gemseo.mlearning.regression.algos.polyreg.PolynomialRegressor]\n  class,\n- Gaussian processes (also known as Kriging),\n  based on the [Scikit-learn](http://scikit-learn.org/stable/) library,\n  for that use the\n  [GaussianProcessRegressor][gemseo.mlearning.regression.algos.gpr.GaussianProcessRegressor]\n  class,\n- Mixture of experts, for that use the\n  [MOERegressor][gemseo.mlearning.regression.algos.moe.MOERegressor] class,\n- Random forest models,\n  based on the [Scikit-learn](http://scikit-learn.org/stable/) library,\n  for that use the\n  [RandomForestRegressor][gemseo.mlearning.regression.algos.random_forest.RandomForestRegressor]\n  class.\n- RBF models (Radial Basis Functions),\n  using the [SciPy](http://scipy.org/) library,\n  for that use the\n  [RBFRegressor][gemseo.mlearning.regression.algos.rbf.RBFRegressor] class.\n- PCE models (Polynomial Chaos Expansion),\n  based on the [OpenTURNS](https://openturns.github.io/www/) library,\n  for that use the\n  [PCERegressor][gemseo.mlearning.regression.algos.pce.PCERegressor] class.\n\nTo understand the detailed behavior of the models, please go to the\ndocumentation of the used packages.\n\n## Extending surrogate models\n\nAll surrogate models work the same way: the\n[BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor] base\nclass shall be extended. See [this page][extend-gemseo-features] to learn how to run\nGEMSEO with external Python modules. Then, the\n[RegressorFactory][gemseo.mlearning.regression.algos.factory.RegressorFactory] can\nbuild the new\n[BaseRegressor][gemseo.mlearning.regression.algos.base_regressor.BaseRegressor]\nautomatically from its regression\nalgorithm name and options. This factory is called by the constructor of\n[SurrogateDiscipline][gemseo.disciplines.surrogate.SurrogateDiscipline].\n\n!!! info \"See also\"\n\n    More generally, GEMSEO provides extension mechanisms to integrate external DOE\n    and optimization algorithms, disciplines, MDAs and surrogate models.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}