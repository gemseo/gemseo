{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Scaling.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nfrom gemseo.algos.design_space import DesignSpace\nfrom gemseo.algos.doe.openturns.openturns import OpenTURNS\nfrom gemseo.algos.optimization_problem import OptimizationProblem\nfrom gemseo.core.mdo_functions.mdo_function import MDOFunction\nfrom gemseo.mlearning.regression.algos.gpr import GaussianProcessRegressor\nfrom gemseo.mlearning.regression.quality.r2_measure import R2Measure\nfrom gemseo.problems.optimization.rosenbrock import Rosenbrock"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scaling data around zero is important to avoid numerical issues\nwhen fitting a machine learning model.\nThis is all the more true as\nthe variables have different ranges\nor the fitting relies on numerical optimization techniques.\nThis example illustrates the latter point.\n\nFirst,\nwe consider the Rosenbrock function $f(x)=(1-x_1)^2+100(x_2-x_1^2)^2$\nover the domain $[-2,2]^2$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = Rosenbrock()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In order to approximate this function with a regression model,\nwe sample it 30 times with an optimized Latin hypercube sampling (LHS) technique\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "opt_lhs = OpenTURNS(\"OT_OPT_LHS\")\nopt_lhs.execute(problem, n_samples=30)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and save the samples in an [IODataset][gemseo.datasets.io_dataset.IODataset]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset_train = problem.to_dataset(opt_naming=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We do the same with a full-factorial design of experiments (DOE) of size 900:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "full_fact = OpenTURNS(\"OT_FULLFACT\")\nfull_fact.execute(problem, n_samples=30 * 30)\ndataset_test = problem.to_dataset(opt_naming=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then,\nwe create a first Gaussian process regressor from the training dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gpr = GaussianProcessRegressor(dataset_train)\ngpr.learn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and compute its R2 quality from the test dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r2 = R2Measure(gpr)\nr2.compute_test_measure(dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then,\nwe create a second Gaussian process regressor from the training dataset\nwith the default input and output transformers that are [MinMaxScaler][gemseo.mlearning.transformers.scaler.min_max_scaler.MinMaxScaler]:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gpr = GaussianProcessRegressor(\n    dataset_train, transformer=GaussianProcessRegressor.DEFAULT_TRANSFORMER\n)\ngpr.learn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that the scaling improves the R2 quality (recall: the higher, the better):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r2 = R2Measure(gpr)\nr2.compute_test_measure(dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We note that in this case, the input scaling does not contribute to this improvement:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gpr = GaussianProcessRegressor(dataset_train, transformer={\"outputs\": \"MinMaxScaler\"})\ngpr.learn()\nr2 = R2Measure(gpr)\nr2.compute_test_measure(dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can also see that using a [StandardScaler][gemseo.mlearning.transformers.scaler.standard_scaler.StandardScaler] is less relevant in this case:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gpr = GaussianProcessRegressor(dataset_train, transformer={\"outputs\": \"StandardScaler\"})\ngpr.learn()\nr2 = R2Measure(gpr)\nr2.compute_test_measure(dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally,\nwe rewrite the Rosenbrock function as $f(x)=(1-x_1)^2+100(0.01x_2-x_1^2)^2$\nand its domain as $[-2,2]\\times[-200,200]$:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "design_space = DesignSpace()\ndesign_space.add_variable(\"x1\", lower_bound=-2, upper_bound=2)\ndesign_space.add_variable(\"x2\", lower_bound=-200, upper_bound=200)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "in order to have inputs with different orders of magnitude.\nWe create the learning and test datasets in the same way:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "problem = OptimizationProblem(design_space)\nproblem.objective = MDOFunction(\n    lambda x: (1 - x[0]) ** 2 + 100 * (0.01 * x[1] - x[0] ** 2) ** 2, \"f\"\n)\nopt_lhs.execute(problem, n_samples=30)\ndataset_train = problem.to_dataset(opt_naming=False)\nfull_fact.execute(problem, n_samples=30 * 30)\ndataset_test = problem.to_dataset(opt_naming=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and build a first Gaussian process regressor with a min-max scaler for the outputs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gpr = GaussianProcessRegressor(dataset_train, transformer={\"outputs\": \"MinMaxScaler\"})\ngpr.learn()\nr2 = R2Measure(gpr)\nr2.compute_test_measure(dataset_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The R2 quality is degraded\nbecause estimating the model's correlation lengths is complicated.\nThis can be facilitated by setting a [MinMaxScaler][gemseo.mlearning.transformers.scaler.min_max_scaler.MinMaxScaler] for the inputs:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "gpr = GaussianProcessRegressor(\n    dataset_train, transformer={\"inputs\": \"MinMaxScaler\", \"outputs\": \"MinMaxScaler\"}\n)\ngpr.learn()\nr2 = R2Measure(gpr)\nr2.compute_test_measure(dataset_test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}