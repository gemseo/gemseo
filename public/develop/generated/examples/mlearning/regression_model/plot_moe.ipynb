{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Mixture of experts.\n\nA [MOERegressor][gemseo.mlearning.regression.algos.moe.MOERegressor] is a mixture of experts for regression purposes.\n\nIn this demo, we load a dataset (the Rosenbrock function in 2D) and apply a\nmixture of experts regression model to obtain an approximation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nimport matplotlib.pyplot as plt\nfrom numpy import array\nfrom numpy import hstack\nfrom numpy import linspace\nfrom numpy import meshgrid\nfrom numpy import nonzero\nfrom numpy import sqrt\nfrom numpy import zeros\n\nfrom gemseo import create_benchmark_dataset\nfrom gemseo.mlearning import create_regression_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Dataset (Rosenbrock)\n\nWe here consider the Rosenbrock function with two inputs, on the interval\n$[-2, 2] \\times [-2, 2]$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load dataset\n\nA prebuilt dataset for the Rosenbrock function with two inputs is given\nas a dataset parametrization, based on a full factorial DOE of the input\nspace with 100 points.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = create_benchmark_dataset(\"RosenbrockDataset\", opt_naming=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Print information\n\nInformation about the dataset can easily be displayed by printing the\ndataset directly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Show dataset\n\nThe dataset object can present the data in tabular form.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Mixture of experts (MoE)\n\nIn this section we load a mixture of experts regression model through the\nmachine learning API, using clustering, classification and regression models.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Mixture of experts model\n\nWe construct the MoE model using the predefined parameters,\nand fit the model to the dataset through the [learn()][gemseo.mlearning.regression.algos.moe.MOERegressor.learn] method.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = create_regression_model(\"MOERegressor\", dataset)\nmodel.set_clusterer(\"KMeans\", n_clusters=3)\nmodel.set_classifier(\"KNNClassifier\", n_neighbors=5)\nmodel.set_regressor(\"GaussianProcessRegressor\")\nmodel.learn()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Tests\n\nHere, we test the mixture of experts method applied to two points:\n(1, 1), the global minimum, where the function is zero, and (-2, -2), an\nextreme point where the function has a high value (max on the domain). The\nclasses are expected to be different at the two points.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "input_value = {\"x\": array([1, 1])}\nanother_input_value = {\"x\": array([[1, 1], [-2, -2]])}\n\nfor _value in [input_value, another_input_value]:\n    for _cls in range(model.n_clusters):\n        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot clusters\n\nHere, we plot the 10x10 = 100 Rosenbrock function data points, with colors\nrepresenting the obtained clusters. The Rosenbrock function is represented\nby a contour plot in the background.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_samples = dataset.n_samples\n# Dataset is based on a DOE of 100=10^2 fullfact.\ninput_dim = int(sqrt(n_samples))\nassert input_dim**2 == n_samples  # Check that n_samples is a square number\n\ncolors = [\"b\", \"r\", \"g\", \"o\", \"y\"]\ninputs = dataset.input_dataset.to_numpy()\noutputs = dataset.output_dataset.to_numpy()\nx = inputs[:input_dim, 0]\ny = inputs[:input_dim, 0]\n\nZ = zeros((input_dim, input_dim))\nfor i in range(input_dim):\n    Z[i, :] = outputs[input_dim * i : input_dim * (i + 1), 0]\nfig = plt.figure()\ncnt = plt.contour(x, y, Z, 50)\nfig.colorbar(cnt)\nfor index in range(model.n_clusters):\n    samples = nonzero(model.labels == index)[0]\n    plt.scatter(inputs[samples, 0], inputs[samples, 1], color=colors[index])\nplt.scatter(1, 1, marker=\"x\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot data and predictions from final model\n\nWe construct a refined input space, and compute the model predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "refinement = 200\n\nfine_x = linspace(x[0], x[-1], refinement)\nfine_y = linspace(y[0], y[-1], refinement)\nfine_x, fine_y = meshgrid(fine_x, fine_y)\nfine_input = {\"x\": hstack([fine_x.flatten()[:, None], fine_y.flatten()[:, None]])}\nfine_z = model.predict(fine_input)\n\n# Reshape\nfine_z = fine_z[\"rosen\"].reshape((refinement, refinement))\n\nplt.figure()\nplt.imshow(Z)\nplt.colorbar()\nplt.title(\"Original data\")\nplt.show()\n\nplt.figure()\nplt.imshow(fine_z)\nplt.colorbar()\nplt.title(\"Predictions\")\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Plot local models\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "for i in range(model.n_clusters):\n    plt.figure()\n    plt.imshow(\n        model.predict_local_model(fine_input, i)[\"rosen\"].reshape((\n            refinement,\n            refinement,\n        ))\n    )\n    plt.colorbar()\n    plt.title(f\"Local model {i}\")\n    plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}