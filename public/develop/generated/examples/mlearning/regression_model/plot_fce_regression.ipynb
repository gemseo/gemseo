{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Function chaos expansion.\n\nGiven a training dataset\nwhose input samples are generated from OpenTURNS probability distributions,\nthe [FCERegressor][gemseo.mlearning.regression.algos.fce.FCERegressor] can use any linear model fitting algorithm,\nincluding sparse techniques,\nto fit a functional chaos expansion (FCE) model of the form\n\n$$Y = \\sum_{i\\in\\mathcal{I}\\subset\\mathbb{N}^d} w_i\\Psi_i(X)$$\n\nwhere $\\Psi_i(X)=\\prod_{j=1}^d\\psi_{i,j}(X_j)$\nand $\\mathbb{E}[\\Psi_i(X)\\Psi_j(X)]=\\delta_{ij}$\nwith $\\delta$ the Kronecker delta and $X$ a random vector.\n\nA particular version of FCE is the polynomial chaos expansion (PCE)\nfor which the class [PCERegressor][gemseo.mlearning.regression.algos.pce.PCERegressor] interfaces\nthe OpenTURNS algorithm `openturns.FunctionalChaosAlgorithm`\n(see the [OpenTURNS documentation](https://openturns.github.io/openturns/latest/user_manual/_generated/openturns.FunctionalChaosAlgorithm.html?highlight=functionalchaosalgorithm#openturns.FunctionalChaosAlgorithm)).\n\nNote that FCE can also learn Jacobian data\nin the hope of improving the quality of the surrogate model\nfor the same evaluation budget.\n\nIn this example,\nwe will compare different types of [FCERegressor][gemseo.mlearning.regression.algos.fce.FCERegressor]\nto approximate the Ishigami function\n\n$$f(X) = \\sin(X_1) + 7\\sin(X_2)^2 + 0.1X_3^4\\sin(X_1)$$\n\nwhere $X_1$, $X_2$ and $X_3$\nare independent and uniformly distributed over the interval $[-\\pi,\\pi]$.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import annotations\n\nfrom numpy import array\n\nfrom gemseo import sample_disciplines\nfrom gemseo.algos.doe.openturns.settings.ot_opt_lhs import OT_OPT_LHS_Settings\nfrom gemseo.algos.doe.scipy.settings.mc import MC_Settings\nfrom gemseo.datasets.dataset import Dataset\nfrom gemseo.mlearning.linear_model_fitting.elastic_net_cv_settings import (\n    ElasticNetCV_Settings,\n)\nfrom gemseo.mlearning.linear_model_fitting.lars_cv_settings import LARSCV_Settings\nfrom gemseo.mlearning.linear_model_fitting.lasso_cv_settings import LassoCV_Settings\nfrom gemseo.mlearning.linear_model_fitting.linear_regression_settings import (\n    LinearRegression_Settings,\n)\nfrom gemseo.mlearning.linear_model_fitting.null_space_settings import NullSpace_Settings\nfrom gemseo.mlearning.linear_model_fitting.omp_cv_settings import (\n    OrthogonalMatchingPursuitCV_Settings,\n)\nfrom gemseo.mlearning.linear_model_fitting.ridge_cv_settings import RidgeCV_Settings\nfrom gemseo.mlearning.linear_model_fitting.spgl1_settings import SPGL1_Settings\nfrom gemseo.mlearning.regression.algos.fce import FCERegressor\nfrom gemseo.mlearning.regression.algos.fce_settings import FCERegressor_Settings\nfrom gemseo.mlearning.regression.algos.fce_settings import OrthonormalFunctionBasis\nfrom gemseo.mlearning.regression.algos.pce import PCERegressor\nfrom gemseo.mlearning.regression.algos.pce_settings import PCERegressor_Settings\nfrom gemseo.mlearning.regression.quality.r2_measure import R2Measure\nfrom gemseo.post.dataset.bars import BarPlot\nfrom gemseo.problems.uncertainty.ishigami.ishigami_discipline import IshigamiDiscipline\nfrom gemseo.problems.uncertainty.ishigami.ishigami_space import IshigamiSpace"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First,\nwe define the Ishigami discipline and its uncertain space:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "discipline = IshigamiDiscipline()\nuncertain_space = IshigamiSpace(IshigamiSpace.UniformDistribution.OPENTURNS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and create a training dataset using an optimized latin hypercube sampling:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "training_dataset = sample_disciplines(\n    [discipline],\n    uncertain_space,\n    \"y\",\n    algo_settings_model=OT_OPT_LHS_Settings(n_samples=70, eval_jac=True),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "as well as a validation dataset using Monte Carlo sampling:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "validation_dataset = sample_disciplines(\n    [discipline],\n    uncertain_space,\n    \"y\",\n    algo_settings_model=MC_Settings(n_samples=1000),\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then,\nwe create standard and gradient-enhanced FCEs\nusing an orthonormal polynomial basis (default basis)\nwith a maximum total degree of 7\nand different regression techniques from scikit-learn to estimate the coefficients,\nnamely [ordinary least squares](https://scikit-learn.org/stable/modules/linear_model.html#ordinary-least-squares),\n[ridge](https://scikit-learn.org/stable/modules/linear_model.html#regression) (i.e., L2 regularisation),\n[lasso](https://scikit-learn.org/stable/modules/linear_model.html#lasso) (i.e., L1 regularisation),\n[elasticnet](https://scikit-learn.org/stable/modules/linear_model.html#elastic-net) (i.e., L1 and L2 regularisation),\n[least angle regression](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression) (LARS)\nand [orthogonal matching pursuit](https://scikit-learn.org/stable/modules/linear_model.html#orthogonal-matching-pursuit-omp).\nNote that all these algorithms have been finely tuned using cross-validation,\nexcept ordinary least squares regression for which there is no parameter to tune.\nWe also add the [SPGL1 algorithm](https://friedlander.io/spgl1)\nto solve a basis pursuit denoise (BPN) problem,\nas well as a null space algorithm.\n\n!!! quote \"References\"\n      Tiziano Ghisu, Diego I. Lopez, Pranay Seshadri and Shahrokh Shahpar.\n      [Gradient-enhanced Least-square Polynomial Chaos Expansions for Uncertainty Quantification and Robust Optimization](https://arc.aiaa.org/doi/abs/10.2514/6.2021-3073).\n      AIAA AVIATION FORUM, 2021.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r2_learning = []\nr2_validation = []\nr2_learning_ge = []\nr2_validation_ge = []\nnull_space_settings = NullSpace_Settings()\nfor linear_model_fitter_settings in [\n    LinearRegression_Settings(),\n    RidgeCV_Settings(),\n    LassoCV_Settings(),\n    ElasticNetCV_Settings(),\n    LARSCV_Settings(),\n    OrthogonalMatchingPursuitCV_Settings(),\n    SPGL1_Settings(sigma=1e-7),\n    null_space_settings,\n]:\n    if linear_model_fitter_settings == null_space_settings:\n        # The null space technique requires gradient observations.\n        r2_learning.append(0.0)\n        r2_validation.append(0.0)\n    else:\n        # Train an FCE.\n        fce_settings = FCERegressor_Settings(\n            degree=7,\n            linear_model_fitter_settings=linear_model_fitter_settings,\n        )\n        fce = FCERegressor(training_dataset, fce_settings)\n        fce.learn()\n\n        # Assess the quality of the FCE.\n        r2 = R2Measure(fce)\n        r2_learning.append(r2.compute_learning_measure().round(2)[0])\n        r2_validation.append(r2.compute_test_measure(validation_dataset).round(2)[0])\n\n    # Train a gradient-enhanced FCE.\n    fce_settings = FCERegressor_Settings(\n        degree=7,\n        linear_model_fitter_settings=linear_model_fitter_settings,\n        learn_jacobian_data=True,\n    )\n    fce = FCERegressor(training_dataset, fce_settings)\n    fce.learn()\n\n    # Assess the quality of the gradient-enhanced FCE.\n    r2 = R2Measure(fce)\n    r2_learning_ge.append(r2.compute_learning_measure().round(2)[0])\n    r2_validation_ge.append(r2.compute_test_measure(validation_dataset).round(2)[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We create also a [PCERegressor][gemseo.mlearning.regression.algos.pce.PCERegressor]\nusing the LARS algorithm implemented in OpenTURNS:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "pce = PCERegressor(training_dataset, PCERegressor_Settings(degree=7, use_lars=True))\npce.learn()\nr2 = R2Measure(pce)\nr2_learning.append(r2.compute_learning_measure().round(2)[0])\nr2_validation.append(r2.compute_test_measure(validation_dataset).round(2)[0])\nr2_learning_ge.append(0)\nr2_validation_ge.append(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "From these results,\nwe can plot the quality of the different surrogate models,\nexpressed in terms of coefficient of determination $R^2$\n(the higher, the better):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = Dataset()\ndataset.add_group(\n    \"R2\",\n    array([r2_learning, r2_validation, r2_learning_ge, r2_validation_ge]),\n    (\"OLS\", \"L2\", \"L1\", \"L1-L2\", \"LARS\", \"OMP\", \"SPGL1\", \"NullSpace\", \"OT-LARS\"),\n)\ndataset.index = [\"Learning\", \"Validation\", \"Learning-GE\", \"Validation-GE\"]\n\nbarplot = BarPlot(dataset, annotate=False)\nbarplot.execute(save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First,\nlet us focus on the standard FCEs\nthat have not learned derivatives (\"Learning\" and \"Validation\" in the legend).\nWe can see that\nthe quality of learning is perfect, regardless of the method.\nThat's good, but not enough.\nBut what interests us is the quality of prediction of the validation dataset\nto see if the surrogate model avoids overfitting.\nIn this regard,\nordinary least squares regression and ridge regression are wrong\nwhile the other techniques are very good,\nwithout really being able to tell them apart.\nNow,\nif we have a look to the gradient-enhanced FCEs\n(\"Learning-GE\" and \"Validation-GE\" in the legend).\nwe can see that the quality is significantly better, except for the LARS method.\n\nLastly,\nthese numerical experiments can be repeated\nby replacing the polynomial basis with the Fourier series.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "r2_learning = []\nr2_validation = []\nr2_learning_ge = []\nr2_validation_ge = []\nnull_space_settings = NullSpace_Settings()\nfor linear_model_fitter_settings in [\n    LinearRegression_Settings(),\n    RidgeCV_Settings(),\n    LassoCV_Settings(),\n    ElasticNetCV_Settings(),\n    LARSCV_Settings(),\n    OrthogonalMatchingPursuitCV_Settings(),\n    SPGL1_Settings(sigma=1e-7),\n    null_space_settings,\n]:\n    if linear_model_fitter_settings == null_space_settings:\n        # The null space technique requires gradient observations.\n        r2_learning.append(0.0)\n        r2_validation.append(0.0)\n    else:\n        # Train an FCE.\n        fce_settings = FCERegressor_Settings(\n            degree=7,\n            linear_model_fitter_settings=linear_model_fitter_settings,\n            basis=OrthonormalFunctionBasis.FOURIER,\n        )\n        fce = FCERegressor(training_dataset, fce_settings)\n        fce.learn()\n\n        # Assess the quality of the FCE.\n        r2 = R2Measure(fce)\n        r2_learning.append(r2.compute_learning_measure().round(2)[0])\n        r2_validation.append(r2.compute_test_measure(validation_dataset).round(2)[0])\n\n    # Train a gradient-enhanced FCE.\n    fce_settings = FCERegressor_Settings(\n        degree=7,\n        linear_model_fitter_settings=linear_model_fitter_settings,\n        basis=OrthonormalFunctionBasis.FOURIER,\n        learn_jacobian_data=True,\n    )\n    fce = FCERegressor(training_dataset, fce_settings)\n    fce.learn()\n\n    # Assess the quality of the gradient-enhanced FCE.\n    r2 = R2Measure(fce)\n    r2_learning_ge.append(r2.compute_learning_measure().round(2)[0])\n    r2_validation_ge.append(r2.compute_test_measure(validation_dataset).round(2)[0])\n\ndataset = Dataset()\ndataset.add_group(\n    \"R2\",\n    array([r2_learning, r2_validation, r2_learning_ge, r2_validation_ge]),\n    (\"OLS\", \"L2\", \"L1\", \"L1-L2\", \"LARS\", \"OMP\", \"SPGL1\", \"NullSpace\"),\n)\ndataset.index = [\"Learning\", \"Validation\", \"Learning-GE\", \"Validation-GE\"]\n\nbarplot = BarPlot(dataset, annotate=False)\nbarplot.execute(save=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then see the same type of ranking, with even better validation qualities.\nThis can be easily explained by the nature of Ishigami's function,\nin which trigonometric terms are important.\nFurthermore,\nlearning Jacobian significantly improves the quality of surrogate models\nin the case of ridge regression and ordinary least squares.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}